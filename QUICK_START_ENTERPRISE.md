# Quick Start - Enterprise Scale Processing

## ğŸš€ Ready to Process Everything!

Your platform now supports **30+ file types** and **10+ enterprise systems**!

## Step 1: Install Dependencies

```bash
# Install core dependencies
pip install -r requirements.txt

# Optional: For better PDF extraction
pip install pdfplumber

# Optional: For image OCR
pip install pytesseract
```

## Step 2: Run Batch Processing

```bash
# Process any directory - auto-detects ALL file types!
python test_batch.py --dir "YOUR_DATA_FOLDER"
```

**That's it!** The system will:
- âœ… Auto-detect file types (PDF, Excel, Word, images, etc.)
- âœ… Process all files in parallel
- âœ… Extract metadata and relationships
- âœ… Generate training data

---

## ğŸ“ Supported File Types (Auto-Detected)

### Documents
- PDF, Word, PowerPoint, Excel, Markdown, Text

### Data Files
- CSV, JSON, XML, YAML, TOML, INI

### Images
- PNG, JPG, GIF, BMP, TIFF, WEBP, SVG

### Archives
- ZIP, TAR, GZ

### Databases
- PostgreSQL, MySQL, SQL Server, SQLite, Oracle, MongoDB

---

## ğŸ¢ Enterprise Systems

### CRM Systems
- **Salesforce**: `salesforce://user:pass@instance.salesforce.com`
- **HubSpot**: `hubspot://api_key`
- **Dynamics**: `dynamics://instance.crm.dynamics.com`

### ERP Systems
- **SAP**: `sap://host:port`
- **Oracle ERP**: `oracleerp://host:port`
- **NetSuite**: `netsuite://account_id`

### Cloud Storage
- **OneDrive/SharePoint**: `onedrive://tenant_id`
- **Google Drive**: `googledrive://project_id`

---

## ğŸ’¡ Example: Process Everything from a Company

```python
from main import DataTransformationPipeline

pipeline = DataTransformationPipeline()

# 1. Process all files in a directory
results = pipeline.process_batch(
    input_directory="acquired_company_data",
    output_directory="output",
    detect_relationships=True
)

# 2. Connect to their Salesforce
salesforce_data = pipeline.process(
    input_path="salesforce://user:pass@instance.salesforce.com",
    output_path="output/salesforce.jsonl"
)

# 3. Connect to their database
db_data = pipeline.process(
    input_path="postgresql://user:pass@host:5432/dbname",
    output_path="output/database.jsonl"
)

# 4. Process OneDrive files
onedrive_data = pipeline.process(
    input_path="onedrive://tenant_id",
    output_path="output/onedrive.jsonl"
)
```

---

## ğŸ“Š Output Structure

```
output/
â”œâ”€â”€ processed/              # Individual processed files
â”‚   â”œâ”€â”€ document1_processed.jsonl
â”‚   â”œâ”€â”€ spreadsheet1_processed.jsonl
â”‚   â””â”€â”€ ...
â”œâ”€â”€ relationships/          # File relationships
â”‚   â””â”€â”€ relationship_graph.json
â”œâ”€â”€ metadata/               # File metadata index
â”‚   â””â”€â”€ file_metadata.json
â””â”€â”€ agentic_ai/            # Training data
    â””â”€â”€ training_data.jsonl
```

---

## âš™ï¸ Configuration

### For Enterprise Systems

Create `.env` file:
```bash
SALESFORCE_USERNAME=user@company.com
SALESFORCE_PASSWORD=password
HUBSPOT_API_KEY=your_key
MS_TENANT_ID=your_tenant
```

### Config File (`config.yaml`):
```yaml
enterprise_connectors:
  salesforce:
    enabled: true
    username: ${SALESFORCE_USERNAME}
    password: ${SALESFORCE_PASSWORD}
  
  hubspot:
    enabled: true
    api_key: ${HUBSPOT_API_KEY}
```

---

## ğŸ¯ What Happens When You Run It

1. **File Scanning**: Finds all files in directory
2. **Type Detection**: Auto-detects file types
3. **Parallel Processing**: Processes files simultaneously
4. **Data Extraction**: Extracts structured data + text
5. **Metadata Extraction**: Author, dates, entities, key terms
6. **Relationship Detection**: Finds connections between files
7. **Training Data**: Generates LLM-ready training data

---

## ğŸ“ˆ Scalability

- âœ… **100+ files**: Handles easily
- âœ… **500GB+**: Processes in batches
- âœ… **Parallel processing**: Uses all CPU cores
- âœ… **Resume capability**: Can resume if interrupted
- âœ… **Error handling**: Continues on individual file errors

---

## ğŸ” Check Results

```bash
# View summary
cat output/summary.json

# View relationships
cat output/relationships/relationship_graph.json

# View training data
head -n 5 output/agentic_ai/training_data.jsonl
```

---

## ğŸ‰ You're Ready!

Just run:
```bash
python test_batch.py --dir "YOUR_DATA"
```

**All file types and enterprise systems are automatically detected and processed!**

