# LEBRON-J'JAMES-TRANSFORMATION-BUSINESS

**Enterprise-Scale Data Transformation Platform for LLM Training**

A comprehensive, production-ready platform that transforms enterprise data from multiple sources (files, databases, CRM, ERP, cloud storage) into LLM-training-ready formats with intelligent relationship detection and agentic AI formatting.

---

## ğŸ¯ Overview

This platform processes enterprise data at scale (100+ files, 500GB+ capacity) through a complete transformation pipeline:

1. **Ingestion** - Extract data from 40+ file types and enterprise systems
2. **Cleaning** - Normalize, deduplicate, and validate data
3. **Redaction** - Detect and redact PII/PHI
4. **Compliance** - Validate against GDPR, HIPAA, PCI-DSS, SOX
5. **Relationship Detection** - Intelligently link related files across systems
6. **Agentic AI Formatting** - Generate multi-file context training data

---

## âœ¨ Key Features

### ğŸ“ **40+ File Type Support**
- **Documents**: PDF, Word, PowerPoint, Excel, Markdown, Text
- **Data**: CSV, JSON, XML, YAML, TOML, INI
- **Images**: PNG, JPG, GIF, BMP, TIFF, WEBP, SVG (with OCR)
- **Archives**: ZIP, TAR, GZIP

### ğŸ—„ï¸ **Enterprise System Connectors**
- **Databases**: PostgreSQL, MySQL, SQL Server, SQLite, Oracle, MongoDB
- **CRM**: Salesforce, HubSpot, Microsoft Dynamics
- **ERP**: SAP, Oracle ERP, NetSuite
- **Cloud**: OneDrive/SharePoint, Google Drive

### ğŸ¤– **Intelligent Features**
- **Batch Processing**: Parallel processing of 100+ files
- **Relationship Detection**: Content-based, filename, and metadata matching
- **Multi-File Context**: Links related files (e.g., Excel â†’ PowerPoint â†’ Word)
- **Agentic AI Training**: Generates training data with relationship awareness
- **Scalable**: Handles enterprise-scale datasets

---

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/LEBRON-J-JAMES-TRANSFORMATION-BUSINESS.git
cd LEBRON-J-JAMES-TRANSFORMATION-BUSINESS

# Install dependencies
pip install -r requirements.txt

# Optional: For better PDF extraction
pip install pdfplumber

# Optional: For image OCR
pip install pytesseract
```

### Basic Usage

```bash
# Process a directory of files
python test_batch.py --dir "YOUR_DATA_FOLDER"
```

### Process Enterprise Systems

```python
from main import DataTransformationPipeline

pipeline = DataTransformationPipeline()

# Process database
result = pipeline.process(
    input_path="postgresql://user:pass@host:5432/dbname",
    output_path="output/db.jsonl"
)

# Process Salesforce
result = pipeline.process(
    input_path="salesforce://user:pass@instance.salesforce.com",
    output_path="output/salesforce.jsonl"
)
```

---

## ğŸ“Š Sample Data

The repository includes sample files from a real test run:

**Input Files:**
- `samples/East West Bancorp Model V2.xlsx` - Excel financial model
- `samples/Sherry_Hu_Resume.docx` - Word document
- `samples/FNP_ATZ_PJ_V2.pptx` - PowerPoint presentation

**Processed Outputs:**
- `samples/East West Bancorp Model V2_processed.jsonl`
- `samples/Sherry_Hu_Resume_processed.jsonl`
- `samples/FNP_ATZ_PJ_V2_processed.jsonl`

These demonstrate the complete transformation pipeline from raw files to LLM-ready training data.

---

## ğŸ“š Documentation

### Quick Guides (Root)
- **[README.md](README.md)** - This file (overview and quick start)
- **[QUICK_START.md](QUICK_START.md)** - Getting started guide
- **[ARCHITECTURE.md](ARCHITECTURE.md)** - System architecture overview

### Detailed Documentation (`/docs`)
- **[Technical Documentation](docs/TECHNICAL.md)** - Deep technical details
- **[Component Guide](docs/COMPONENTS.md)** - What each part of the codebase does
- **[API Reference](docs/API.md)** - API documentation
- **[Enterprise Connectors](docs/ENTERPRISE_CONNECTORS.md)** - Enterprise system integration
- **[Relationship Detection](docs/RELATIONSHIPS.md)** - How relationship detection works
- **[Agentic AI Formatting](docs/AGENTIC_AI.md)** - Training data generation

### Guides
- **[Batch Processing Guide](BATCH_PROCESSING_GUIDE.md)** - How to process multiple files
- **[Enterprise Connectors Guide](ENTERPRISE_CONNECTORS_GUIDE.md)** - Enterprise system setup
- **[How to Add Your Data](HOW_TO_ADD_YOUR_DATA.md)** - Data preparation guide

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Data Transformation Pipeline              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                     â”‚                     â”‚
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
   â”‚Ingestionâ”‚          â”‚  Cleaning â”‚         â”‚ Redaction  â”‚
   â”‚  Layer  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   Layer   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   Layer   â”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
        â”‚                     â”‚                     â”‚
        â”‚              â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”                â”‚
        â”‚              â”‚Compliance â”‚                â”‚
        â”‚              â”‚   Layer   â”‚                â”‚
        â”‚              â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                â”‚
        â”‚                     â”‚                     â”‚
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
   â”‚            Relationship Detection                     â”‚
   â”‚  (Content-based, Filename, Metadata Matching)        â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚         Agentic AI Formatting                         â”‚
   â”‚  (Multi-file context, Training data generation)      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                  â”‚   Output    â”‚
                  â”‚  (JSONL)    â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Components:**
- **Ingestion**: 40+ file handlers + enterprise connectors
- **Processing**: Cleaning, redaction, compliance checking
- **Intelligence**: Relationship detection, metadata extraction
- **Output**: LLM-ready training data with multi-file context

---

## ğŸ“ Project Structure

```
LEBRON-J-JAMES-TRANSFORMATION-BUSINESS/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/          # File format handlers
â”‚   â”‚   â”œâ”€â”€ excel_handler.py
â”‚   â”‚   â”œâ”€â”€ pdf_handler.py
â”‚   â”‚   â”œâ”€â”€ word_handler.py
â”‚   â”‚   â”œâ”€â”€ enterprise_connectors.py
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ cleaning/           # Data cleaning pipeline
â”‚   â”œâ”€â”€ redaction/          # PII/PHI detection & redaction
â”‚   â”œâ”€â”€ compliance/         # Regulatory compliance checking
â”‚   â”œâ”€â”€ batch/              # Batch processing
â”‚   â”œâ”€â”€ relationships/      # Relationship detection
â”‚   â””â”€â”€ structuring/        # LLM formatting
â”œâ”€â”€ samples/                # Sample input/output files
â”œâ”€â”€ docs/                   # Detailed documentation
â”œâ”€â”€ main.py                 # Main pipeline orchestrator
â”œâ”€â”€ test_batch.py           # Batch processing test script
â””â”€â”€ requirements.txt        # Python dependencies
```

---

## ğŸ”§ Configuration

### Environment Variables

Create `.env` file:
```bash
# Salesforce
SALESFORCE_USERNAME=user@company.com
SALESFORCE_PASSWORD=password

# HubSpot
HUBSPOT_API_KEY=your_api_key

# Microsoft Graph (OneDrive)
MS_TENANT_ID=your_tenant_id
MS_CLIENT_ID=your_client_id
MS_CLIENT_SECRET=your_secret
```

### Config File (`config.yaml`)

```yaml
batch:
  max_workers: 4
  chunk_size: 100

relationships:
  min_confidence: 0.7
  strategies:
    - content
    - filename
    - metadata

redaction:
  strategy: mask
  entities: [EMAIL, PHONE, SSN]
```

---

## ğŸ“ˆ Performance

- **Throughput**: Processes 100+ files in parallel
- **Scale**: Handles 500GB+ datasets
- **Speed**: Parallel processing with progress tracking
- **Resume**: Can resume interrupted batch jobs
- **Error Handling**: Graceful error handling per file

---

## ğŸ§ª Testing

```bash
# Run batch processing test
python test_batch.py --dir "test_data"

# Run single file test
python test_poc.py
```

---

## ğŸ“ License

MIT License - See [LICENSE](LICENSE) file for details.

---

## ğŸ¤ Contributing

This is a private enterprise project. For questions or contributions, please contact the repository owner.

---

## ğŸ“ Support

For technical questions or issues:
1. Check the [documentation](docs/)
2. Review [sample files](samples/)
3. Contact the repository owner

---

## ğŸ¯ Use Cases

- **Company Acquisitions**: Process all data from acquired companies
- **Data Migration**: Transform legacy data for modern systems
- **LLM Training**: Generate training data from enterprise documents
- **Compliance**: Ensure data meets regulatory requirements
- **Data Integration**: Unify data from multiple enterprise systems

---

## ğŸš€ Roadmap

- [x] Batch processing (100+ files)
- [x] Relationship detection
- [x] Enterprise connectors
- [x] Agentic AI formatting
- [ ] Distributed processing (Dask/Spark)
- [ ] Web UI
- [ ] Real-time processing
- [ ] Advanced analytics

---

**Built for enterprise scale. Production ready.** ğŸš€
